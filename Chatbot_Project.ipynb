{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Copy of Chatbot Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shravani1291/ShapeAI_Shravani_G/blob/main/Chatbot_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T13:40:28.582020Z",
          "iopub.execute_input": "2021-10-07T13:40:28.582716Z",
          "iopub.status.idle": "2021-10-07T13:40:33.014868Z",
          "shell.execute_reply.started": "2021-10-07T13:40:28.582627Z",
          "shell.execute_reply": "2021-10-07T13:40:33.013585Z"
        },
        "trusted": true,
        "id": "GB9y8Gch0T0w"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T13:40:33.016592Z",
          "iopub.execute_input": "2021-10-07T13:40:33.016847Z",
          "iopub.status.idle": "2021-10-07T13:40:33.034729Z",
          "shell.execute_reply.started": "2021-10-07T13:40:33.016813Z",
          "shell.execute_reply": "2021-10-07T13:40:33.033189Z"
        },
        "trusted": true,
        "id": "FcUgAHtR0T05"
      },
      "source": [
        "\"\"\"\n",
        "INPUTS -> Encoder -> ENC OUTPUTS, THOUGHT VECTOR -> \n",
        "\n",
        "Attention Network -> Attention Weights (x ENC OUTPUTS) -> ATTENTION OUTPUT\n",
        "\n",
        "ATTENTION OUTPUT, ACTUAL OUTPUT(INPUT) -> DECODER -> FINAL OUTPUT\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "ENCODER ARCHITECTURE:-\n",
        "\n",
        "\n",
        "INPUTS -> EMBEDDING -> GRU\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "ATTENTION NETWORK ARCHITECTURE\n",
        "\n",
        "ENC OUTPUTS     -> ENC LAYER     -> --------       \n",
        "                                            ------> ACTIVATION -> FINAL LAYER -> ATTENTION WEIGHTS\n",
        "THOUGHT VECTOR  -> THOUGHT LAYER -> --------\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T13:40:33.036119Z",
          "iopub.execute_input": "2021-10-07T13:40:33.036715Z",
          "iopub.status.idle": "2021-10-07T13:40:33.049775Z",
          "shell.execute_reply.started": "2021-10-07T13:40:33.036678Z",
          "shell.execute_reply": "2021-10-07T13:40:33.048818Z"
        },
        "trusted": true,
        "id": "c2IkIUEI0T06"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding, encoder_units, batch_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = encoder_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "    \n",
        "    def call(self, inputs, hidden_state):\n",
        "        embedded_inputs = self.embedding(inputs)\n",
        "        enc_outputs, thought_vector = self.gru(embedded_inputs, initial_state=hidden_state)\n",
        "        return enc_outputs, thought_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T13:40:33.054821Z",
          "iopub.execute_input": "2021-10-07T13:40:33.055039Z",
          "iopub.status.idle": "2021-10-07T13:40:33.071723Z",
          "shell.execute_reply.started": "2021-10-07T13:40:33.055015Z",
          "shell.execute_reply": "2021-10-07T13:40:33.070483Z"
        },
        "trusted": true,
        "id": "6-SydAbq0T08"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.enc_output_layer = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        self.thought_layer    = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        self.final_layer      = tf.keras.layers.Dense(1    , kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        \n",
        "    def call(self, enc_outputs, thought_vector):\n",
        "        thought_matrix = tf.expand_dims(thought_vector, 1)\n",
        "        \n",
        "        scores = self.final_layer(tf.keras.activations.tanh(self.enc_output_layer(enc_outputs) + self.thought_layer(thought_matrix)))\n",
        "        attention_weights = tf.keras.activations.softmax(scores, axis=-1)\n",
        "        \n",
        "        attention_output = attention_weights * enc_outputs # Shape (batch_size, num_outputs, output_size)\n",
        "        attention_output = tf.reduce_sum(attention_output, axis=1) # New shape (batch_size, output_size)\n",
        "        \n",
        "        return attention_output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T13:40:33.074371Z",
          "iopub.execute_input": "2021-10-07T13:40:33.075076Z",
          "iopub.status.idle": "2021-10-07T13:40:33.095314Z",
          "shell.execute_reply.started": "2021-10-07T13:40:33.075011Z",
          "shell.execute_reply": "2021-10-07T13:40:33.094250Z"
        },
        "trusted": true,
        "id": "yiPMKaDn0T09"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding, decoder_units, batch_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units = decoder_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        \n",
        "        self.attention = Attention(self.dec_units)\n",
        "        self.word_output = tf.keras.layers.Dense(vocab_size, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, thought_vector):\n",
        "        attention_output, attention_weights = self.attention(enc_outputs, thought_vector)\n",
        "        #Shape of attention output (batch_size, size_of_embedding)\n",
        "        \n",
        "        embedded_inputs = self.embedding(inputs) #Shape (batch_size, num_words, size_of_embedding)\n",
        "        attention_output = tf.expand_dims(attention_output, 1) #Shape of attention output (batch_size, 1, size_of_embedding)\n",
        "        concat_inputs = tf.concat([attention_output, embedded_inputs], axis=-1)\n",
        "        \n",
        "        decoder_outputs, hidden_state = self.gru(concat_inputs) #Shape (batch_size, 1, size_of_embedding)\n",
        "        decoder_outputs = tf.reshape(decoder_outputs, (-1, decoder_outputs.shape[2])) #Shape (batch_size, size_of_embedding)\n",
        "        \n",
        "        final_outputs = self.word_output(decoder_outputs)\n",
        "        return final_outputs, hidden_state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T13:40:33.100926Z",
          "iopub.execute_input": "2021-10-07T13:40:33.101406Z",
          "iopub.status.idle": "2021-10-07T13:40:33.121285Z",
          "shell.execute_reply.started": "2021-10-07T13:40:33.101370Z",
          "shell.execute_reply": "2021-10-07T13:40:33.120362Z"
        },
        "trusted": true,
        "id": "22uj5Tlj0T1A"
      },
      "source": [
        "class Train:\n",
        "    def __init__(self):\n",
        "        self.optimizer = tf.keras.optimizers.Adam()\n",
        "        self.base_loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "        \n",
        "    def loss_function(self, y_real, y_pred):\n",
        "        base_mask = tf.math.logical_not(tf.math.equal(y_real, 0))\n",
        "        base_loss = self.base_loss_function(y_real, y_pred)\n",
        "        \n",
        "        mask = tf.cast(base_mask, dtype=base_loss.dtype)\n",
        "        final_loss = mask * base_loss\n",
        "        \n",
        "        return tf.reduce_mean(final_loss)\n",
        "    \n",
        "    def train_step(self, train_data, label_data, enc_hidden, encoder, decoder, batch_size, label_tokenizer):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_outputs, thought_vector = encoder(train_data, enc_hidden)\n",
        "            dec_hidden = thought_vector\n",
        "            dec_input = tf.expand_dims([label_tokenizer.word_index['<start>']] * batch_size, 1)\n",
        "            \n",
        "            for index in range(1, label_data.shape[1]):\n",
        "                outputs, dec_hidden, _ = decoder(dec_input, enc_outputs, dec_hidden)\n",
        "                \n",
        "                dec_input = tf.expand_dims(label_data[:, index], 1)\n",
        "                loss = loss + self.loss_function(label_data[:, index], outputs)\n",
        "        \n",
        "        word_loss = loss / int(label_data.shape[1])\n",
        "        \n",
        "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        return word_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T13:40:33.125944Z",
          "iopub.execute_input": "2021-10-07T13:40:33.129083Z",
          "iopub.status.idle": "2021-10-07T13:40:33.140341Z",
          "shell.execute_reply.started": "2021-10-07T13:40:33.129045Z",
          "shell.execute_reply": "2021-10-07T13:40:33.139372Z"
        },
        "trusted": true,
        "id": "Wgmd8sZY0T1B"
      },
      "source": [
        "\"\"\"\n",
        "[he she it name this that these those their you]\n",
        "\n",
        "y_real = [0, 0, 0, 0, '1', 0, 0, 0, 0, 0]\n",
        "\n",
        "math.equal() = [True, True, True, True, False, True, True, True, True, True]\n",
        "logical not = \n",
        "mask = [1, 1, 1, 1, '0', 1, 1, 1, 1, 1]\n",
        "\n",
        "[0.001, 0.001, 0.001, 0.001, '0.9', 0.001, 0.001, 0.001, 0.001, 0.002]\n",
        "\n",
        "\n",
        "[1, 1, 1, 1, '0', 1, 1, 1, 1, 1] * [0.001, 0.001, 0.001, 0.001, '-0.1', 0.001, 0.001, 0.001, 0.001, 0.002]\n",
        "\n",
        "final_loss = [0.001, 0.001, 0.001, 0.001, 0, 0.001, 0.001, 0.001, 0.001, 0.002]\n",
        "\n",
        "return 0.0013\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T13:40:33.145106Z",
          "iopub.execute_input": "2021-10-07T13:40:33.147802Z",
          "iopub.status.idle": "2021-10-07T13:40:33.161327Z",
          "shell.execute_reply.started": "2021-10-07T13:40:33.147769Z",
          "shell.execute_reply": "2021-10-07T13:40:33.160439Z"
        },
        "trusted": true,
        "id": "_2pR11Qw0T1D"
      },
      "source": [
        "class Data_Preprocessing:\n",
        "    def __init__(self):\n",
        "        self.temp = None\n",
        "    \n",
        "    def get_data(self, path):\n",
        "        file = open(path, 'r').read()\n",
        "        lists = [f.split('\\t') for f in file.split('\\n')]\n",
        "        \n",
        "        questions = [x[0] for x in lists]\n",
        "        answers = [x[1] for x in lists]\n",
        "        \n",
        "        return questions, answers\n",
        "    \n",
        "    def process_sentence(self, line):\n",
        "        line = line.lower().strip()\n",
        "        \n",
        "        line = re.sub(r\"([?!.,])\", r\" \\1 \", line)\n",
        "        line = re.sub(r'[\" \"]+', \" \", line)\n",
        "        line = re.sub(r\"[^a-zA-Z?!.,]+\", \" \", line)\n",
        "        line = line.strip()\n",
        "        \n",
        "        line = '<start> ' + line + ' <end>'\n",
        "        return line\n",
        "    \n",
        "    def word_to_vec(self, inputs):\n",
        "        tokenizer = Tokenizer(filters='')\n",
        "        tokenizer.fit_on_texts(inputs)\n",
        "        \n",
        "        vectors = tokenizer.texts_to_sequences(inputs)\n",
        "        vectors = pad_sequences(vectors, padding='post')\n",
        "        \n",
        "        return vectors, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T13:40:33.165714Z",
          "iopub.execute_input": "2021-10-07T13:40:33.168588Z",
          "iopub.status.idle": "2021-10-07T13:40:33.725204Z",
          "shell.execute_reply.started": "2021-10-07T13:40:33.168551Z",
          "shell.execute_reply": "2021-10-07T13:40:33.724510Z"
        },
        "trusted": true,
        "id": "Ij5-w0G20T1E"
      },
      "source": [
        "data = Data_Preprocessing()\n",
        "\n",
        "questions, answers = data.get_data('../input/shapeai-chatbot/chatbot.txt')\n",
        "\n",
        "questions = [data.process_sentence(str(sentence)) for sentence in questions]\n",
        "answers = [data.process_sentence(str(sentence)) for sentence in answers]\n",
        "\n",
        "train_vectors, train_tokenizer = data.word_to_vec(questions)\n",
        "label_vectors, label_tokenizer = data.word_to_vec(answers)\n",
        "\n",
        "max_length_train = train_vectors.shape[1]\n",
        "max_length_label = label_vectors.shape[1]\n",
        "\n",
        "batch_size = 64\n",
        "buffer_size = train_vectors.shape[0]\n",
        "embedding_dim = 256\n",
        "steps_per_epoch = buffer_size//batch_size\n",
        "units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T13:40:33.728693Z",
          "iopub.execute_input": "2021-10-07T13:40:33.728956Z",
          "iopub.status.idle": "2021-10-07T13:40:33.734172Z",
          "shell.execute_reply.started": "2021-10-07T13:40:33.728926Z",
          "shell.execute_reply": "2021-10-07T13:40:33.731711Z"
        },
        "trusted": true,
        "id": "UnV2nxEB0T1G"
      },
      "source": [
        "vocab_train = len(train_tokenizer.word_index) + 1\n",
        "vocab_label = len(label_tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T13:40:33.735728Z",
          "iopub.execute_input": "2021-10-07T13:40:33.736119Z",
          "iopub.status.idle": "2021-10-07T13:40:35.397109Z",
          "shell.execute_reply.started": "2021-10-07T13:40:33.736084Z",
          "shell.execute_reply": "2021-10-07T13:40:35.395614Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "9oPRI5Y10T1I"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((train_vectors, label_vectors))\n",
        "dataset = dataset.shuffle(buffer_size)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T13:40:35.398355Z",
          "iopub.execute_input": "2021-10-07T13:40:35.398747Z",
          "iopub.status.idle": "2021-10-07T13:40:35.703989Z",
          "shell.execute_reply.started": "2021-10-07T13:40:35.398705Z",
          "shell.execute_reply": "2021-10-07T13:40:35.703267Z"
        },
        "trusted": true,
        "id": "3pTppq_p0T1J"
      },
      "source": [
        "encoder = Encoder(vocab_train, embedding_dim, units, batch_size)\n",
        "decoder = Decoder(vocab_label, embedding_dim, units, batch_size)\n",
        "trainer = Train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T13:40:35.705402Z",
          "iopub.execute_input": "2021-10-07T13:40:35.705775Z",
          "iopub.status.idle": "2021-10-07T13:49:08.172011Z",
          "shell.execute_reply.started": "2021-10-07T13:40:35.705738Z",
          "shell.execute_reply": "2021-10-07T13:49:08.171221Z"
        },
        "trusted": true,
        "id": "g3PqeEQ90T1K"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    enc_hidden = tf.zeros((batch_size, units))\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch_num, (train_data, label_data)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = trainer.train_step(train_data, label_data, enc_hidden, encoder, decoder, batch_size, label_tokenizer)\n",
        "        total_loss = total_loss + batch_loss\n",
        "        \n",
        "    print(f\"Epoch: {epoch}, Loss: {total_loss/steps_per_epoch}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T14:27:10.477265Z",
          "iopub.execute_input": "2021-10-07T14:27:10.477549Z",
          "iopub.status.idle": "2021-10-07T14:27:10.490139Z",
          "shell.execute_reply.started": "2021-10-07T14:27:10.477515Z",
          "shell.execute_reply": "2021-10-07T14:27:10.489280Z"
        },
        "trusted": true,
        "id": "ygmQWhVq0T1K"
      },
      "source": [
        "class Chatbot:\n",
        "    def __init__(self, encoder, decoder, train_tokenizer, label_tokenizer, max_length_train, units):\n",
        "        self.train_tokenizer = train_tokenizer\n",
        "        self.label_tokenizer = label_tokenizer\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.units = units\n",
        "        self.data = Data_Preprocessing()\n",
        "        self.maxlen = max_length_train\n",
        "    \n",
        "    def clean_answer(self, answer):\n",
        "        answer = answer[:-1]\n",
        "        answer = ' '.join(answer)\n",
        "        return answer\n",
        "    \n",
        "    def predict(self, sentence):\n",
        "        sentence = self.data.process_sentence(sentence)\n",
        "        \n",
        "        sentence_mat = []\n",
        "        for word in sentence.split(\" \"):\n",
        "            try:\n",
        "                sentence_mat.append(self.train_tokenizer.word_index[word])\n",
        "            except:\n",
        "                return \"Could not understand that, can you re-phrase?\"\n",
        "        \n",
        "        sentence_mat = pad_sequences([sentence_mat], maxlen=self.maxlen, padding='post')\n",
        "        sentence_mat = tf.convert_to_tensor(sentence_mat)\n",
        "        \n",
        "        enc_hidden = [tf.zeros((1, self.units))]\n",
        "        encoder_outputs, thought_vector = self.encoder(sentence_mat, enc_hidden)\n",
        "        \n",
        "        dec_hidden = thought_vector\n",
        "        dec_input = tf.expand_dims([label_tokenizer.word_index['<start>']], 0)\n",
        "        \n",
        "        answer = []\n",
        "        for i in range(1, self.maxlen):\n",
        "            pred, dec_hidden, _ = decoder(dec_input, encoder_outputs, dec_hidden)\n",
        "            \n",
        "            word = self.label_tokenizer.index_word[np.argmax(pred[0])]\n",
        "            answer.append(word)\n",
        "            \n",
        "            if word == '<end>':\n",
        "                return self.clean_answer(answer)\n",
        "            \n",
        "            dec_input = tf.expand_dims([np.argmax(pred[0])], 0)\n",
        "        \n",
        "        return self.clean_answer(answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T14:27:10.737628Z",
          "iopub.execute_input": "2021-10-07T14:27:10.738190Z",
          "iopub.status.idle": "2021-10-07T14:27:10.742595Z",
          "shell.execute_reply.started": "2021-10-07T14:27:10.738154Z",
          "shell.execute_reply": "2021-10-07T14:27:10.741584Z"
        },
        "trusted": true,
        "id": "WDrzrRcv0T1L"
      },
      "source": [
        "bot = Chatbot(encoder, decoder, train_tokenizer, label_tokenizer, max_length_train, units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-07T14:33:48.858766Z",
          "iopub.execute_input": "2021-10-07T14:33:48.859295Z",
          "iopub.status.idle": "2021-10-07T14:34:23.002604Z",
          "shell.execute_reply.started": "2021-10-07T14:33:48.859260Z",
          "shell.execute_reply": "2021-10-07T14:34:23.001711Z"
        },
        "trusted": true,
        "id": "5SZa5ZK90T1M"
      },
      "source": [
        "question = ''\n",
        "while True:\n",
        "    question = str(input('You:'))\n",
        "    if question == 'quit' or question == 'Quit':\n",
        "        break\n",
        "        \n",
        "    answer = bot.predict(question)\n",
        "    print(f'Bot: {answer}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgbJr1PA0T1N"
      },
      "source": [
        "pred = [[1,2,3,4,5]]\n",
        "pred[0] = [1,2,3,4,5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwzlQN410T1N"
      },
      "source": [
        "sentence - english sentence\n",
        "\n",
        "remove things from it \n",
        "convert it to one hot form\n",
        "pass it through the whole model and get the predictions"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}